{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f180a9d2",
   "metadata": {},
   "source": [
    "# Sign Language Recognition Demo\n",
    "\n",
    "This jupyter notebook is a quick Sign Language Recognition Demo. Please follow along to train a Sign Language Recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!pip3 install memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b8d5d2",
   "metadata": {},
   "source": [
    "## Feature Extraction Pipeline\n",
    "\n",
    "This part of the notebook covers feature extraction from raw videos. We assume that our raw videos are sourced from the ASL Capture App. This app records whole sessions of the user signing words. Each session consists of 20 signs for 20 different words. The raw video must therefore be split up and then have mediapipe features extracted before we can train the models. The mediapipe features are then converted into HTK features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ec306",
   "metadata": {},
   "source": [
    "### Mediapipe Feature Extraction\n",
    "\n",
    "To extract mediapipe features, we must first add the raw videos to `videos/` and the run the following commands. You can find the raw videos at this Google Drive link: https://drive.google.com/file/d/1_sImmOjPiflbV7TWDzTiHs1W1qMF3DtY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c704e7",
   "metadata": {},
   "source": [
    "We can count the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dfb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_video_files = !ls videos/\n",
    "print(len(sliced_video_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594d2d0",
   "metadata": {},
   "source": [
    "We can also list the sliced up videos. Note the naming convention. The elements in the name are delimited by a hyphen (`-`). The first element is the user ID set in the Capture App. The second element is the word being signed. The third element is the dat and time at which the session was recorded (`YYYY_MM_DD_HH_MM_SS`). The fourth and last element, before the file extension, is the session ID followed by the number of times the sign was recorded in that session (`0` indexed). Ignore the error directory listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa98a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls videos/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5358f1a2",
   "metadata": {},
   "source": [
    "Finally, we can play any videos listed above. To do so, define the `video_file` variable below as the name of the video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import Video\n",
    "\n",
    "video_dir = 'videos'\n",
    "video_file = '00001-sleep-2022_09_19_15_42_00.307-18.mp4'\n",
    "\n",
    "video_path = os.path.join(video_dir, video_file)\n",
    "Video(video_path, height=540, width=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf4d0ee",
   "metadata": {},
   "source": [
    "In the next step, we extract mediapipe features from the split videos and store them in `mediapipe/`. To do this, we call `mediapipe_convert.py`. This script generates mediapipe features for the split videos and stores them in the output directory. The directory hierarchy under the output directory is organized like so: The first level contains a directory for each user named: `[Capture App User ID]-singlesign/`. Beneath this level, the script creates a directory for each signed word named: `[sign]/`. Finally, at the last level the script creates a directory named after the date and time of each session in which the sign was recorded. The naming generally looks like this: `[YYYY_MM_DD_HH_MM_SS]` This final directory contains a data file with the mediapipe features.\n",
    "\n",
    "Thia step can take a while. You can skip it by downloading and extracting the tarball at the following link: https://drive.google.com/file/d/1opuR5k8AwmoivuOBHePvT9_HhRYhJmT1/view?usp=sharing.\n",
    "\n",
    "The link above contains features for all the videos linked in the README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358bf0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mediapipe_convert import main\n",
    "\n",
    "# This is system dependent, so set it appropriately. Setting a large number can crash the kernel.\n",
    "num_threads = 4\n",
    "source_dir = 'videos/'\n",
    "dest_dir = 'mediapipe/'\n",
    "\n",
    "%mprun main(noMark=True, inputDirectory=source_dir, outputDirectory=dest_dir, num_threads=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab46178",
   "metadata": {},
   "source": [
    "We can count the number of signs for which we have mediapipe features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea20f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls mediapipe/00001-singlesign/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbcb0b4",
   "metadata": {},
   "source": [
    "For the final data preparation step, we convert our mediapipe features into a format usable by the HTK toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8abaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.utils import load_json\n",
    "from prepare_data import prepare_data\n",
    "\n",
    "# This file should contain a path near the end thatpoints to the mediapipe features above.\n",
    "features_file = 'configs/features.json'\n",
    "\n",
    "# Load features\n",
    "raw_data = load_json(features_file)\n",
    "    \n",
    "prepare_data(raw_data, isSingleWord=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b7a1f",
   "metadata": {},
   "source": [
    "## Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce7708",
   "metadata": {},
   "source": [
    "Run the train command below to train HMM models. The `train_iters` arg determines how many iterations to train to, while `n_splits` determines the number of folds for cross validation. Pass `gmm_mix` as a value that is not `None` to train GMM Mixture Models for emission probabilities. Pass `middle` to the `gmm_pattern` variable to use GMM Mixtures for all but the start/end states. Pass `all` to include GMM Mixtures for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75345e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "train_iters = [150]\n",
    "leave_one_out = False\n",
    "n_splits = 20\n",
    "hmm_step_type = 'single'\n",
    "gmm_mix = 3\n",
    "gmm_pattern = 'middle'\n",
    "random_state = 423\n",
    "\n",
    "train(\n",
    "    train_iters=train_iters,\n",
    "    leave_one_out=leave_one_out, # n_splits would be ignored in this case\n",
    "    n_splits=n_splits,\n",
    "    hmm_step_type=hmm_step_type, # see utils/model_utils.py for possible values\n",
    "    gmm_mix=gmm_mix,\n",
    "    gmm_pattern=gmm_pattern, # If gmm mix is none this is ignored.\n",
    "    random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78113474",
   "metadata": {},
   "source": [
    "## Testing Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce68a8",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The next cell runs the same trained HMMs from the previous step on all of the test data. It outputs a weighted average on results across all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_results, clean_and_make_path, get_num_lines\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "## We can run HVite and HResults on the entire test dataset.\n",
    "\n",
    "all_results = {'error': [], 'num_rows': []}\n",
    "\n",
    "for i in range(n_splits):\n",
    "    fold = str(i)\n",
    "    \n",
    "    clean_and_make_path(f'results/{fold}')\n",
    "    clean_and_make_path(f'hresults/{fold}')\n",
    "    \n",
    "    model_file = f'models/{fold}/hmm150/newMacros'\n",
    "    test_data = f'lists/{fold}/test.data'\n",
    "    results_file = f'results/{fold}/res_hmm150.mlf'\n",
    "    hresults_file = f'hresults/{fold}/res_hmm150.txt'\n",
    "    \n",
    "    # HVite\n",
    "    HVite_str = (f'HVite -A -H {model_file} -m -S {test_data} -i '\n",
    "                    f'{results_file} -p -200 -w wordNet.txt -s 25 dict wordList')\n",
    "\n",
    "    os.system(HVite_str)\n",
    "\n",
    "    # HResults\n",
    "    HResults_str = (f'HResults -A -h -e \\\\?\\\\?\\\\? sil0 -e \\\\?\\\\?\\\\? '\n",
    "                        f'sil1 -p -t -I all_labels.mlf wordList {results_file} '\n",
    "                        f'>> {hresults_file}')\n",
    "    os.system(HResults_str)\n",
    "    \n",
    "    results = get_results(hresults_file)\n",
    "    num_rows = get_num_lines(test_data)\n",
    "    \n",
    "    all_results['error'].append(results['error'])\n",
    "    all_results['num_rows'].append(num_rows)\n",
    "\n",
    "total = sum(all_results['num_rows'])\n",
    "print(\"\\nError: \", np.average(all_results['error'], weights=all_results['num_rows']))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e96f3",
   "metadata": {},
   "source": [
    "### Video Samples\n",
    "\n",
    "The section below contains some video samples from the test data. We play the video with the correct label (from the trained HMMs) on the left side and the incorrect label on the right side. The first cell displays two different words, while the second two cells display the same word, but with a correct and incorrect label, on the left and right respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbeff1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Look is displayed on the left and is labeled correctly by the HMM (which is shown below)\n",
    "## Talk is displayed on the right and is labeled incorrect by the HMM (which is shown below)\n",
    "\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from IPython import display\n",
    "\n",
    "videos = [\n",
    "    'videos/00001-puzzle-2022_09_19_15_42_00.307-17.mp4',\n",
    "    'videos/00001-puzzle-2022_09_19_15_42_00.307-10.mp4'\n",
    "]\n",
    "grid = GridspecLayout(1, len(videos))\n",
    "\n",
    "for i, videopath in enumerate(videos):\n",
    "    out = Output()\n",
    "    with out:\n",
    "        display.display(display.Video(videopath, embed=True, height=540, width=360))\n",
    "    grid[0,i] = out\n",
    "\n",
    "grid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e098f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
